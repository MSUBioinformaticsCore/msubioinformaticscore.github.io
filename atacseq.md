---
layout: post
title: "Running nf-core/atacseq on MSU HPCC"
date: 2024-11-03
author: John Vusich, Leah Terrian
categories: jekyll update
---

## Overview

The **MSU HPCC**, managed by ICER, provides an efficient and scalable environment for running complex bioinformatics analyses. This tutorial will guide you through running the **nf-core/atac-seq** pipeline on the HPCC, ensuring reproducibility and optimal performance.

## Key Benefits of nf-core/atacseq

**nf-core/atac-seq** is designed for:

- **Reproducible ATAC-seq Analysis**: Provides robust, community-curated workflows.
- **Portability**: Runs seamlessly across different computing environments.
- **Scalability**: Capable of processing small- to large-scale ATAC-seq datasets.

## Prerequisites

- Access to MSU HPCC with a valid ICER account.
- Basic knowledge of **Singularity** and **Nextflow** module usage.

## Step-by-Step Tutorial

### Note on Directory Variables

On the MSU HPCC:

- `$HOME` automatically routes to the user's home directory (`/mnt/home/username`).
- `$SCRATCH` automatically routes to the user's scratch directory, which is ideal for temporary files and large data processing.

### Note on Working Directory

The working directory, which stores intermediate and temporary files, can be specified separately using the `-w` flag when running the pipeline. This helps keep your analysis outputs and temporary data organized.

### 1. Load Nextflow Module

Ensure that **Nextflow** is available in your environment:

```bash
module load Nextflow
```

### 2. Create an Analysis Directory

Set up a dedicated directory for your analysis (referred to as the Analysis Directory):

```bash
mkdir $HOME/atacseq_project
cd $HOME/atacseq_project
```

- Modify `$HOME/atacseq_project` to better suit your project description, if needed.

### 3. Prepare Sample Sheet

Create a sample sheet (`samplesheet.csv`) with the following format:

```csv
sample,fastq_1,fastq_2,replicate
sample1,/path/to/sample1_R1.fastq.gz,/path/to/sample1_R2.fastq.gz,1
sample2,/path/to/sample2_R1.fastq.gz,/path/to/sample2_R2.fastq.gz,1
```

Ensure all paths to the FASTQ files are correct.

### 4. Configure ICER Environment

Create a `nextflow.config` file to run the pipeline with SLURM:

```groovy
process {
    executor = 'slurm'
}
```

### 5. Create Bash Script

Create a `submit_atacseq_job.sh` file. You can copy and paste the below script, but note that you will have to modify the `--outdir`, `--fasta`, and `--gtf` to match your output and reference genome paths.
This is a typical shell script for submitting an **nf-core/atacseq** job to SLURM:

```bash
#!/bin/bash --login

#SBATCH --job-name=atacseq_job
#SBATCH --time=24:00:00
#SBATCH --mem=24GB
#SBATCH --cpus-per-task=8

cd $HOME/atacseq_project
module load Nextflow/23.10.0

nextflow pull nf-core/atacseq
nextflow run nf-core/atacseq -r 2.1.2 --read_length 150 --input ./samplesheet.csv -profile singularity --outdir ./atacseq_results --fasta ./Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz --gtf ./Homo_sapiens.GRCh38.108.gtf.gz -work-dir $SCRATCH/atacseq_work -c ./nextflow.config
```

- Modify `--outdir`, `--fasta`, and `--gtf` to match your output and reference genome paths.
- Modify `--read_length` to match the number of base pairs per read in your fastq files (commonly = 100 or 150).

### 6. Run Bash Script with SLURM
In the terminal:

```bash
sbatch submit_atacseq_job.sh
```
- Note: This job will likely take several hours to complete.

### 7. Monitor and Manage the Run

- Use `squeue` or `sacct` to check the job status. I.e. "squeue -u username".
- Verify the output in the specified results directory.

## Note on Reference Genomes

Common reference genomes can be found in the /mnt/research/common-data/Bio/ folder on the HPCC. You can find guidance on finding reference genomes on the HPCC or downloading them from Ensembl in thisÂ [GitHub repository](https://github.com/johnvusich/reference-genomes).

Execute the pipeline with the following command. This example includes a `-w` flag to specify a working directory in the user's scratch space for intermediate files:

```bash
nextflow run nf-core/atacseq -profile singularity --input samplesheet.csv --genome GRCh38 -c nextflow.config -w $SCRATCH/atacseq_project
```

- The `-profile singularity` flag ensures that **Singularity** containers are used.
- Modify `--genome` to match your reference genome.
- Modify `-w $SCRATCH/atacseq_project` to better suit your project description, if needed.

## Best Practices

- **Check Logs**: Regularly inspect log files generated by the pipeline for any warnings or errors.
- **Resource Allocation**: Adjust the `icer.config` to optimize resource usage based on dataset size.
- **Storage Management**: Ensure adequate storage space for intermediate and final results.

## Getting Help

If you encounter any issues or have questions while running **nf-core/atacseq** on the HPCC, consider the following resources:

- **nf-core Community**: Visit the [nf-core website](https://nf-co.re) for documentation, tutorials, and community support.
- **ICER Support**: Contact ICER consultants through the [MSU ICER support page](https://icer.msu.edu/contact) for assistance with HPCC-specific questions.
- **Slack Channel**: Join the **nf-core** Slack channel for real-time support and to engage with other users and developers.
- **Nextflow Documentation**: Refer to the [Nextflow documentation](https://www.nextflow.io/docs/latest/index.html) for more details on workflow customization and troubleshooting.

## Conclusion

Running **nf-core/atac-seq** on the MSU HPCC is streamlined with **Singularity** and **Nextflow** modules. This setup supports reproducible, efficient, and large-scale ATAC-seq analyses. By following this guide, you can take full advantage of the HPCC's computing power for your bioinformatics projects.

